}
Y_d = rbind(matrix(0,N*p,N), diag(sqrt(small_sig2/lambda)))
for (i in 1:p){
Y_d[(i-1)*N+1:i*N, ] <- diag(Stack_AR_coeff[i,]*repmat(1,N,1)*sqrt(small_sig))/lambda
}
X_d = rbind(kron(diag(1:P) , diag(sqrt(small_sig2) / lambda)) , zeros(N,K-1))
X_d[size(X_d,1)+1,K] = 1e-10
Y_d <- rbind(Y_d,zeros(1,N))
# Do least square to get posterior
Y_star = rbind(Y,Y_d)
X_star = rbind(X,X_d)
# Get posterior mode/mean of VAR coefficients
phi = mldivide(t(X_star) %*% X_star , t(X_star) %*% Y_star)
e = Y - X %*% phi
e_star = Y_star - X_star %*% phi
# Posterior mode/mean of covariance matrix
SIGMA = mrdivide((t(e_star) %*% e_star), (size(Y_star,1) - size(phi,1)))
# rearrange phi and X to just make constant in first row
phi = rbind(phi[nrow(phi),],phi[1:nrow(phi)-1,])
X = cbind(X[,ncol(X)],X[,1:ncol(X)-1])
return(phi)
}
library(pracma)
library(tidyverse)
BVAR<-function(y,p,lambda){
T=dim(y)[1]
N=dim(y)[2]
#backcast data with the mean
y<-rbind(repmat(colMeans(y),p,1),y)
K=p*N+1
#create data matrics
Y<-y[-1:-(p),]
X=c()
for(i in 1:p){
rnmb<-p+1-i
cnmb<-nrow(y)-i
Z<-y[rnmb:cnmb,]
X<-cbind(X,Z)
}
# constant to stack at last
X<-cbind(X,ones(T,1))
# This is to set prior
small_sig2=zeros(N,1)
Stack_AR_coeff=zeros(p,N)
for(i in c(1:N)){
#calculate variance for each equation to set pror
#by fitting an AR(4) per equation
c(~,small_sig2[i,1])=olsvar(y[,i],4)
ARcoeff = olsvar(y[,i],1)
if (ARcoeff >= 0.8){
Stack_AR_coeff(1,i) = 1}
# run an AR(1) to set to random walk if time series is persistant
# (>0.8)
}
Y_d = rbind(matrix(0,N*p,N), diag(sqrt(small_sig2/lambda)))
for (i in 1:p){
Y_d[(i-1)*N+1:i*N, ] <- diag(Stack_AR_coeff[i,]*repmat(1,N,1)*sqrt(small_sig))/lambda
}
X_d = rbind(kron(diag(1:P) , diag(sqrt(small_sig2) / lambda)) , zeros(N,K-1))
X_d[size(X_d,1)+1,K] = 1e-10
Y_d <- rbind(Y_d,zeros(1,N))
# Do least square to get posterior
Y_star = rbind(Y,Y_d)
X_star = rbind(X,X_d)
# Get posterior mode/mean of VAR coefficients
phi = mldivide(t(X_star) %*% X_star , t(X_star) %*% Y_star)
e = Y - X %*% phi
e_star = Y_star - X_star %*% phi
# Posterior mode/mean of covariance matrix
SIGMA = mrdivide((t(e_star) %*% e_star), (size(Y_star,1) - size(phi,1)))
# rearrange phi and X to just make constant in first row
phi = rbind(phi[nrow(phi),],phi[1:nrow(phi)-1,])
X = cbind(X[,ncol(X)],X[,1:ncol(X)-1])
return(phi)
}
for(i in c(1:p)){
Z=y[c(p+1-i:nrow(y)-i),]
X<-cbind(X,Z)
}
BVAR<-function(y,p,lambda){
T=dim(y)[1]
N=dim(y)[2]
#backcast data with the mean
y<-rbind(repmat(colMeans(y),p,1),y)
K=p*N+1
#create data matrics
Y<-y[-1:-(p),]
X=c()
for(i in 1:p){
rnmb<-p+1-i
cnmb<-nrow(y)-i
Z<-y[rnmb:cnmb,]
X<-cbind(X,Z)
}
# constant to stack at last
X<-cbind(X,ones(T,1))
# This is to set prior
small_sig2=zeros(N,1)
Stack_AR_coeff=zeros(p,N)
for(i in c(1:N)){
#calculate variance for each equation to set pror
#by fitting an AR(4) per equation
c(~,small_sig2[i,1])=olsvar(y[,i],4)
ARcoeff = olsvar(y[,i],1)
if (ARcoeff >= 0.8){
Stack_AR_coeff(1,i) = 1}
# run an AR(1) to set to random walk if time series is persistant
# (>0.8)
}
Y_d = rbind(matrix(0,N*p,N), diag(sqrt(small_sig2/lambda)))
for (i in 1:p){
Y_d[(i-1)*N+1:i*N, ] <- diag(Stack_AR_coeff[i,]*repmat(1,N,1)*sqrt(small_sig))/lambda
}
X_d = rbind(kron(diag(1:P) , diag(sqrt(small_sig2) / lambda)) , zeros(N,K-1))
X_d[size(X_d,1)+1,K] = 1e-10
Y_d <- rbind(Y_d,zeros(1,N))
# Do least square to get posterior
Y_star = rbind(Y,Y_d)
X_star = rbind(X,X_d)
# Get posterior mode/mean of VAR coefficients
phi = mldivide(t(X_star) %*% X_star , t(X_star) %*% Y_star)
e = Y - X %*% phi
e_star = Y_star - X_star %*% phi
# Posterior mode/mean of covariance matrix
SIGMA = mrdivide((t(e_star) %*% e_star), (size(Y_star,1) - size(phi,1)))
# rearrange phi and X to just make constant in first row
phi = rbind(phi[nrow(phi),],phi[1:nrow(phi)-1,])
X = cbind(X[,ncol(X)],X[,1:ncol(X)-1])
return(phi)
}
BVAR<-function(y,p,lambda){
T=dim(y)[1]
N=dim(y)[2]
#backcast data with the mean
y <- rbind(repmat(colMeans(y),p,1),y)
K=p*N+1
#create data matrics
Y<-y[-1:-(p),]
X=c()
for(i in 1:p){
rnmb<-p+1-i
cnmb<-nrow(y)-i
Z<-y[rnmb:cnmb,]
X<-cbind(X,Z)
}
# constant to stack at last
X<-cbind(X,ones(T,1))
# This is to set prior
small_sig2=zeros(N,1)
Stack_AR_coeff=zeros(p,N)
for(i in c(1:N)){
#calculate variance for each equation to set pror
#by fitting an AR(4) per equation
c(~,small_sig2[i,1])=olsvar(y[,i],4)
ARcoeff = olsvar(y[,i],1)
if (ARcoeff >= 0.8){
Stack_AR_coeff(1,i) = 1}
# run an AR(1) to set to random walk if time series is persistant
# (>0.8)
}
Y_d = rbind(matrix(0,N*p,N), diag(sqrt(small_sig2/lambda)))
for (i in 1:p){
Y_d[(i-1)*N+1:i*N, ] <- diag(Stack_AR_coeff[i,]*repmat(1,N,1)*sqrt(small_sig))/lambda
}
X_d = rbind(kron(diag(1:P) , diag(sqrt(small_sig2) / lambda)) , zeros(N,K-1))
X_d[size(X_d,1)+1,K] = 1e-10
Y_d <- rbind(Y_d,zeros(1,N))
# Do least square to get posterior
Y_star = rbind(Y,Y_d)
X_star = rbind(X,X_d)
# Get posterior mode/mean of VAR coefficients
phi = mldivide(t(X_star) %*% X_star , t(X_star) %*% Y_star)
e = Y - X %*% phi
e_star = Y_star - X_star %*% phi
# Posterior mode/mean of covariance matrix
SIGMA = mrdivide((t(e_star) %*% e_star), (size(Y_star,1) - size(phi,1)))
# rearrange phi and X to just make constant in first row
phi = rbind(phi[nrow(phi),],phi[1:nrow(phi)-1,])
X = cbind(X[,ncol(X)],X[,1:ncol(X)-1])
print(phi)
}
BVAR<-function(y,p,lambda){
T=dim(y)[1]
N=dim(y)[2]
#backcast data with the mean
y <- rbind(repmat(colMeans(y),p,1),y)
K=p*N+1
#create data matrics
Y<-y[-1:-(p),]
X=c()
for(i in 1:p){
rnmb<-p+1-i
cnmb<-nrow(y)-i
Z<-y[rnmb:cnmb,]
X<-cbind(X,Z)
}
# constant to stack at last
X<-cbind(X,ones(T,1))
# This is to set prior
small_sig2=zeros(N,1)
Stack_AR_coeff=zeros(p,N)
for(i in c(1:N)){
#calculate variance for each equation to set pror
#by fitting an AR(4) per equation
c(~,small_sig2[i,1])=olsvar(y[,i],4)
ARcoeff = olsvar(y[,i],1)
if (ARcoeff >= 0.8){
Stack_AR_coeff(1,i) = 1}
# run an AR(1) to set to random walk if time series is persistant
# (>0.8)
}
Y_d = rbind(matrix(0,N*p,N), diag(sqrt(small_sig2/lambda)))
for (i in 1:p){
Y_d[(i-1)*N+1:i*N, ] <- diag(Stack_AR_coeff[i,]*repmat(1,N,1)*sqrt(small_sig))/lambda
}
X_d = rbind(kron(diag(1:P) , diag(sqrt(small_sig2) / lambda)) , zeros(N,K-1))
X_d[size(X_d,1)+1,K] = 1e-10
Y_d <- rbind(Y_d,zeros(1,N))
# Do least square to get posterior
Y_star = rbind(Y,Y_d)
X_star = rbind(X,X_d)
# Get posterior mode/mean of VAR coefficients
phi = mldivide(t(X_star) %*% X_star , t(X_star) %*% Y_star)
e = Y - X %*% phi
e_star = Y_star - X_star %*% phi
# Posterior mode/mean of covariance matrix
SIGMA = mrdivide((t(e_star) %*% e_star), (size(Y_star,1) - size(phi,1)))
# rearrange phi and X to just make constant in first row
phi = rbind(phi[nrow(phi),],phi[1:nrow(phi)-1,])
X = cbind(X[,ncol(X)],X[,1:ncol(X)-1])
print(phi)
}
# generate the total amount of the data
rawdata <- as.matrix(alldata) # Remove the NA values due to the loading of the data.
logdata <- log(rawdata)
d4logdata <- 100*(logdata[5:nrow(logdata),]-logdata[1:(nrow(logdata)-4),]) %>% as.matrix()
View(rawdata)
# generate the total amount of the data
rawdata <- as.matrix(alldata) # Remove the NA values due to the loading of the data.
logdata <- log(rawdata)
d4logdata <- 100*(logdata[5:nrow(logdata),]-logdata[1:(nrow(logdata)-4),]) %>% as.matrix()
library(matrixStats)
library(ggploot2)
library(matrixStats)
library(ggplot2)
library(zoo)
library(tidyverse)
library(dplyr)
library(stats)
library(readxl)
library(pracma)
library(tidyverse)
alldata <- readr::read_csv("ABSemp.csv")
# generate the total amount of the data
rawdata <- as.matrix(alldata) # Remove the NA values due to the loading of the data.
logdata <- log(rawdata)
d4logdata <- 100*(logdata[5:nrow(logdata),]-logdata[1:(nrow(logdata)-4),]) %>% as.matrix()
summary(d4logdata)  # check summary to see if data are read correctly
N=dim(d4logdata)[2]
p=4
lambda=0.2 # shrinkage
maxhor=2 # maximum forecast horizon where we are imposing conditions
View(d4logdata)
phi <- read.csv("phi.csv",header=FALSE) %>% as.matrix #estimated parameters produced by MATLAB code
n = nrow(d4logdata)
k = ncol(d4logdata)
# For users, you have to replace the numbers phi if you really want to build up another type of model
# The phi is produced using MATLAB MAIN.m file, please run that file to generate the estimated coefficients.
# Here the phi are divided to give four lags and one constant.
# phi[1,j] for constant, the rest are multiplied by the four lags and do the estimation
rawhat = matrix(0,n,k)
yhat = matrix(0,n,k)
sdiff = matrix(0,n,k)
i=5
for (j in 1:(k-1)){
yhat[i,j]= phi[1,j] + t(d4logdata[(i-1),]) %*% phi[2:86,j] + t(d4logdata[(i-2),]) %*% phi[87:171,j] + t(d4logdata[(i-3),]) %*% phi[172:256,j] + t(d4logdata[(i-4),]) %*%
phi[257:341,j] # y_t = y_{t-1}+y_{t-1}+y{t-3}+y_{t-4}
}
rawhatv = rawdata[(i-4),1:(k-1)] * exp(yhat[i,1:(k-1)]/100) # back transform
yhat[i,k]=100*log(sum(rawhatv)/rawdata[(i-4),k])
rawhat[i,] = rawdata[(i-4),1:k] * exp(yhat[i,1:k]/100)
sdiff[i,] = exp(yhat[i,1:k]/100)
sum(rawhatv)
rawdata[(i-4),k]
# Elvis Yang
# June 2022
# This is the program computes the multipliers and contrafactural or scenario forecasting
library(matrixStats)
library(ggplot2)
library(zoo)
library(tidyverse)
library(dplyr)
library(stats)
library(readxl)
library(pracma)
library(tidyverse)
# This function will estimate a VAR with a constant using least squares with options of
#various methods of bootstraping
BVAR <- function(y,p,lambda){
T=dim(y)[1]
N=dim(y)[2]
#backcast data with the mean
y <- rbind(repmat(colMeans(y),p,1),y)
K=p*N+1
#create data matrics
Y<-y[-1:-(p),]
X=c()
for(i in 1:p){
rnmb<-p+1-i
cnmb<-nrow(y)-i
Z<-y[rnmb:cnmb,]
X<-cbind(X,Z)
}
# constant to stack at last
X<-cbind(X,ones(T,1))
# This is to set prior
small_sig2=zeros(N,1)
Stack_AR_coeff=zeros(p,N)
for(i in c(1:N)){
#calculate variance for each equation to set pror
#by fitting an AR(4) per equation
c(~,small_sig2[i,1])=olsvar(y[,i],4)
alldata <- readr::read_csv("ABSemp.csv")
# generate the total amount of the data
rawdata <- as.matrix(alldata) # Remove the NA values due to the loading of the data.
logdata <- log(rawdata)
d4logdata <- 100*(logdata[5:nrow(logdata),]-logdata[1:(nrow(logdata)-4),]) %>% as.matrix()
summary(d4logdata)  # check summary to see if data are read correctly
N=dim(d4logdata)[2]
p=4
lambda=0.2 # shrinkage
maxhor=2 # maximum forecast horizon where we are imposing conditions
# generate the phi of your estimated model
#phi <- BVAR(d4logdata,p,lambda)
phi <- read.csv("phi.csv",header=FALSE) %>% as.matrix #estimated parameters produced by MATLAB code
n = nrow(d4logdata)
k = ncol(d4logdata)
# For users, you have to replace the numbers phi if you really want to build up another type of model
# The phi is produced using MATLAB MAIN.m file, please run that file to generate the estimated coefficients.
# Here the phi are divided to give four lags and one constant.
# phi[1,j] for constant, the rest are multiplied by the four lags and do the estimation
rawhat = matrix(0,n,k)
yhat = matrix(0,n,k)
sdiff = matrix(0,n,k)
for (i in 5:n){
for (j in 1:(k-1)){
yhat[i,j]= phi[1,j] + t(d4logdata[(i-1),]) %*% phi[2:86,j] + t(d4logdata[(i-2),]) %*% phi[87:171,j] + t(d4logdata[(i-3),]) %*% phi[172:256,j] + t(d4logdata[(i-4),]) %*%
phi[257:341,j] # y_t = y_{t-1}+y_{t-1}+y{t-3}+y_{t-4}
}
rawhatv = rawdata[(i-4),1:(k-1)] * exp(yhat[i,1:(k-1)]/100) # back transform
yhat[i,k]=100*log(sum(rawhatv)/rawdata[(i-4),k])
rawhat[i,] = rawdata[(i-4),1:k] * exp(yhat[i,1:k]/100)
sdiff[i,] = exp(yhat[i,1:k]/100)
}
# Perform out for sample forecast use the rawhat(estimation) to do
# Steps are similar use the last four observation to fit the phi above to generate t+1 forecast
# Testing model using error measurements MAPE and Scaled errors
newraw <- rawdata[9:nrow(rawdata),] # Define the actual values of forecast
rawhat1 <- rawhat[5:nrow(rawhat),]
error <- (newraw - rawhat1)
train_err <- matrix(0,n,k)
# Scale-dependent errors
MAE = mean(abs(error))
RMSE = sqrt(mean(error))
# Percentage error
for(i in 1:(n-4)){
for (j in 1:k){
train_err[i,j] <- 100 * as.vector(error[i,j]) / as.vector(newraw[i,j]) # Percentage error
}
}
# To calculate the MAPE we apply the formula as   sum(abs(y_t - \hat{y_t}/y_t))/(n)
MAPE = sum(abs(train_err))/(n-4 * k )
# Scaled error -- Hyndman & Koehler(2006) see [https://otexts.com/fpp3/accuracy.html]
# First: Define the scaled error yt - y_{t-4} as sdiff and calculated in previous steps
# sdiff = yt - y_{t-4}
# Second, calculate the denominator as for seasonal time series m=4
denom = sum(sdiff)/(n-4)
# Third calculate the q_j
qj = matrix(0,n,k)
for(a in 1:i){
for (b in 1:k){
qj[a,b] = error[a,b]/denom
}
}
MASE = mean(abs(qj))
RMSSE = sqrt(mean(qj^2))
# Conduct our multiplier analysis
multiraw = matrix(0,44,k) # ELVIS -- why here we set the number equals to 44
multiraw[1:4,] = rawdata[(nrow(rawdata)-3):nrow(rawdata),]
multig = matrix(0,44,k)
multilevels = matrix(0,44,k-1) # matrix that saves the evolution of total employment in reaction to 1 percent point shock in each sector
multigrowth = matrix(0,44,k-1) # matrix that saves the evolution of employment growth in reaction to 1 percentage point shock in each sector
for (sector in 1:(k-1)){
for (i in 5:44){
for (j in 1:(k-1)){
multig[i,j]= multig[(i-1),]%*%phi[(2:86),j] + multig[(i-2),]%*%phi[87:171,j] + multig[(i-3),] %*% phi[172:256,j] + multig[(i-4),] %*% phi[257:341,j]
if (i==5 & j==sector) {multig[i,j]=multig[i,j]+1}
}
multiraw[i,1:(k-1)] = multiraw[(i-4),1:(k-1)]*exp(multig[i,1:(k-1)]/100)
multiraw[i,k] = sum(multiraw[i,1:(k-1)])
multig[i,k]=100*log(multiraw[i,k]/multiraw[(i-4),k])
}
multilevels[,sector]=multiraw[,k]
multigrowth[,sector]=multig[,k]
}
multipliers = colSums(multigrowth)/4 # division by 4 is necessary because of seasonal differences
mpers = colCumsums(multigrowth[5:44,])/4 #colCumsums() is a function in matrixStats package
shares=colSums(rawdata[(nrow(rawdata)-3):nrow(rawdata),1:84])/sum(rawdata[(nrow(rawdata)-3):nrow(rawdata),85]) #sector shares estimated like this to eliminate the effect of seasonality
write.csv(file="multipliers.csv",rbind(shares,mpers))
multiplier <- rbind(shares,mpers)
View(multiplier)
library(fpp3)
# Chunk 1
library(fpp3)
# Chunk 2
employ<- read.csv("ABSemp.csv")
View(employ)
View(employ)
ts_employ <- employ |>
mutate(Quarter = yearquarter(my(Date))) |>
select(-Date) |>
as_tsibble(index = Quarter)
View(ts_employ)
ts_employ <- ts_employ |>
filter(Quarter = "2019 Q4")
ts_employ <- ts_employ |>
filter(Quarter == "2019 Q4")
ts_employ <- ts_employ |>
filter(Quarter <= "2019 Q4")
View(ts_employ)
ts_employ <- ts_employ |>
filter(Quarter <= "2019 Q4")
ts_employ <- ts_employ |>
filter(Quarter <= "2019 Q4")
ts_employ <- ts_employ |>
filter(Quarter <= "2019 Q4")
ts_employ <- ts_employ |>
filter(Quarter <= yearquarter("2019 Q4"))
View(ts_employ)
fc_model <- ts_emplong |>
model(stepwise = ARIMA(Employment))
# Chunk 1
library(fpp3)
# Chunk 2
employ<- read.csv("ABSemp.csv")
# Chunk 3
# employ <- employ |>
#   mutate(Month = yearmonth(Date)) |>
#   select(-Date)
ts_employ <- employ |>
mutate(Quarter = yearquarter(my(Date))) |>
select(-Date) |>
as_tsibble(index = Quarter)
ts_employ <- ts_employ |>
filter(Quarter <= yearquarter("2019 Q4"))
# Chunk 4
ts_emplong <- ts_employ |>
pivot_longer(cols = c(-Quarter) , names_to = "Industry", values_to = "Employment")
fc_model <- ts_emplong |>
model(stepwise = ARIMA(Employment))
fc_emp<- fc_model |>
forecast(h=10)
View(fc_model)
test <- ts_employ[21,]
test <- ts_employ[-21,]
ts_employ <- ts_employ |>
filter(Quarter <= yearquarter("2019 Q4"))
# Chunk 1
library(fpp3)
# Chunk 2
employ<- read.csv("ABSemp.csv")
# employ <- employ |>
#   mutate(Month = yearmonth(Date)) |>
#   select(-Date)
ts_employ <- employ |>
mutate(Quarter = yearquarter(my(Date))) |>
select(-Date) |>
as_tsibble(index = Quarter)
pre_2019 <- ts_employ |>
filter(Quarter <= yearquarter("2019 Q4"))
training <- pre_2019[-21,]
View(training)
training <- pre_2019[1:120,]
training <- pre_2019[1:120,]
test <- pre_2019[121:nrow(pre2019)]
test <- pre_2019[121:nrow(pre2019)]
test <- pre_2019[121:nrow(pre_2019)]
test <- pre_2019[121:nrow(pre_2019),]
# Chunk 1
library(fpp3)
# Chunk 2
employ<- read.csv("ABSemp.csv")
# Chunk 3
# employ <- employ |>
#   mutate(Month = yearmonth(Date)) |>
#   select(-Date)
ts_employ <- employ |>
mutate(Quarter = yearquarter(my(Date))) |>
select(-Date) |>
as_tsibble(index = Quarter)
pre_2019 <- ts_employ |>
filter(Quarter <= yearquarter("2019 Q4"))
training <- pre_2019[1:120,]
test <- pre_2019[121:nrow(pre_2019),]
training <- training |>
pivot_longer(cols = c(-Quarter) , names_to = "Industry", values_to = "Employment")
fc_model <- training |>
model(stepwise = ARIMA(Employment))
fc_emp<- fc_model |>
forecast(h=10)
View(fc_model)
View(fc_emp)
