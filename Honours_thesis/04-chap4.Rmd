---
chapter: 4
knit: "bookdown::render_book"
---

# Methdology
## Proposed Model

I plan to use a Bayesian VARX model based on a method proposed by @anderson2020. In the model, each sector is affected by the lags of sectoral growth and a lag of the total employment growth. The lag of aggregate employment growth acts as an economy-wide factor which will affect each sector.

Under the assumption that the structure of the Australian economy will not change during COVID-19, we suggest the BVAR model as

$$
\begin{aligned}
\textbf{y}_t=\textbf{c}+\textbf{A}_1 \textbf{y}_{t-1}+\textbf{A}_2\textbf{y}_{t-2}+\textbf{A}_3\textbf{y}_{t-3}+\textbf{A}_4\textbf{y}_{t-4}+\boldsymbol{\Gamma}\textbf{x}_{t-1}+\bf{u}_t
\end{aligned}
$$

where $\bf{y}_t$ is an $87\times1$ vector of two-digit subsectoral employment growth rate at time $t$  and $\bf{x}_{t-1}$ is the $4\times1$ vector of 4 lags of the growth rate of aggregate employment (this vector of variables are predetermined at time $t$), $\textbf{c}$ is a vector of constants, $\bf{A}_{1,2,3,4}$ are $87\times87$ parameter matrices. $\boldsymbol{\Gamma}$ is a $87\times4$ matrix and $\bf{u}_t$ is a vector of reduced form errors with the mean equals to zero and independent variance $\bf{u}_t \sim (0,\boldsymbol{\Sigma})$. (see Appendix)

## Prior and shrinkage

Bayesian VAR helps to overcome the curse of high dimensionality via the imposition of prior beliefs on the parameters [@banbura2010large]. I will estimate the VARX using Bayesian methods by specifying a Minnesota prior [e.g. @anderson2020; @litterman1986; @robertson1999vector]. In order to set up the Minnesota prior in our BVAR model, @banbura2010large suggests a Minnesota-type prior that applies shrinkage to the VAR slope coefficients as follows:

$$
\begin{aligned}\label{eq:1}
&E[a_{i}^{jk}] = E[\gamma_{i}^j]=0\\
\\
&Var[a_i^{jk}]= 
\begin{cases}
\frac{\lambda^2}{i^2},&j=k\\
\frac{\lambda^2}{i^2}\frac{\sigma^2_{j}}{\sigma^2_k},& otherwise
\end{cases}\\
\\
&Var[\gamma_i^{j}]=\frac{\lambda^2}{i^2}\frac{\sigma^2_{j}}{\sigma^2_e}
\end{aligned}
$$
where the degree of shrinkage is governed by $\lambda$, $\frac{1}{i^2}$ to down-weight more distant lags and the $\frac{\sigma_j^2}{\sigma_k^2}$ adjusts for different scale of the data. $\sigma^2_e$ is the variance after fitting an AR model on total employment growth. 

@banbura2010large also suggested that a natural conjugate Normal-Inverse-Wishart which retains the principle of Minnesota prior will help in adding Minnesota prior to the Bayesian VAR system. Its posterior moments can be calculated either analytically or through adding the dummy observations. I will use dummy observations to estimate the BVAR [@banbura2010large]. More details are provided in the Appendix.


\newpage


## Shrinkage parameter ($\lambda$) selection

Specifically, the Minnesota type prior have the following beliefs about the variances: 

$$
\begin{aligned}
&Var[a_i^{jk}]= 
\begin{cases}
\frac{\lambda^2}{i^2},&j=k\\
\frac{\lambda^2}{i^2}\frac{\sigma^2_{j}}{\sigma^2_k},& otherwise
\end{cases}\\
\\
&Var[\gamma_i^{j}]=\frac{\lambda^2}{i^2}\frac{\sigma^2_{j}}{\sigma^2_e}
\end{aligned}
$$

where $\lambda$ is a shrinkage hyperparemeter here we need to specify based on how far we will shrink the estimator. 





In the VAR literature, the selection of shrinkage hyperparameter $\lambda$ is important in improving forecast accuracy. For example,  @banbura2010large point out that a gain in efficiency could be made by applying an Bayesian shrinkage in estimating large multivariate VAR models. Moreover, they also conclude that large VARs with shrinkage are credible to conduct structural analysis. Nonetheless, the main problem that encountered is to set an appropriate value for $\lambda$ as models become larger (i.e. how far it will shrink the estimators). 


From the above equation, we can see a small shrinkage parameter $\lambda$ will tighten the distribution of prior, and vice versa. Based on applied experiences, Litterman constructed a 40-variable model and concluded that the shrinkage estimate $\lambda=0.2$ is indeed sufficient to deal with many empirical cases [@litterman1986]. At the same time, I should also avoid over-fitting when preserving the most important information from the data. That is, the data size is also an essential basis to be considered when deciding the degree of shrinkage[@banbura2010large]. Unlike the one-digit level (19 sectors) did by @anderson2020, the two-digit level (87 sectors) employment in Australia is more complex and have more variables. Thus, a new shrinkage parameter $\lambda$ should be specified particularly for this case.   



In this section, I will evaluate the efficient shrinkage estimator $\lambda$ with a more sophisticated version of training/test algorithm, called time series cross-validation. The time-series cross validation is an automated program based on a rolling forecasting origin, which measure the performance of a model in a statistical way while avoiding over-fitting the data. Here, the data is split into training/test with a minimum training set of length $n=20$. Then, the program will increasing the size of successive training set by iteration step $hor=1$ and accumulate the error for each step. After that, it will calculate the mean of accumulated error, which will be the training results (see Figure \ref{fig:tscv}). 


\graphicspath{ {/Users/elvisyang/Desktop/hon_proj/Disaggregated_Employment/Honours_thesis/figures} }


\begin{figure}[t]
\includegraphics[scale=0.5]{tscv}
\centering
\caption{Time series cross validation with Blue area is training set and Orange is test set, with iteration step = 1 (Source https://otexts.com/fpp3/tscv.html)}
\label{fig:tscv}
\end{figure}






Apart from this, we will use several famous error measures to select an appropriate shrinkage parameter $\lambda$: 


*Scale-dependent error*



+ Mean absolute error: $MAE = mean(|e_t|)$


+ Root mean squared error: RMSE = $\sqrt{mean(e_t^2)}$



In the real world, MAE and RMSE are commonly used, but only when data all have the same units. They are easy to understand and calculate, which makes them popular to compare forecast accuracy. Nevertheless, they have a significant drawback, which is not unit-free. 



*Percentage error :* 


+ Mean absolute percentage error: $MAPE = mean(|\frac{e_t}{y_t}|)$ 





The percentage error (i.e. MAPE) has the advantage of being unit-free. However, it will become unreliable when the data have zeros or extreme values (i.e. very close to zero). As a result, I will use it only as a reference in this literature.




*Scale-independent error:* 


+ MASE: $MASE = mean(|q_j|)$

+ RMSSE = $RMSSE=\sqrt{mean(q_j^2)}$

where 



$q_j^2=\frac{e_j}{\frac{1}{T-m}\Sigma^T_{t=m+1}|y_t-y_{t-m}|}$ and $m$ is the seasonality. In this case, we set $m=1$ after a seasonal difference and logarithm transformation, which is used for stationary (i.e. non-seasonal) data. 



MASE and RMSSE are useful when comparing forecast accuracy across data with different units [@hyndman2006]. Both of them are scaled based on the same measure but from a simple forecast method. Since the employment data have a strong seasonality, the scaled error is defined using random walk with drift forecasts.




\vspace{24pt}









Table \ref{testres} demostrates the training results for $\lambda$ with the following values $0.3,0.2,0.19 \cdots 0.14$ and $0.1$ after using the time series cross validation algorithm 

\begin{table}[h]
\begin{center}
\begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
\hline\hline
\multicolumn{6}{|c|}{Training results using time-series cross validation}\\
\hline
\multicolumn{6}{|c|}{initial training (n = 20) ; iteration step (hor = 1)}\\
\hline\hline
 $\lambda$    & MAE         & MAPE    & MASE   & RMSE    & RMSSE  \\
\hline\hline
0.3  & 1.0832 e+03 & 14.5901 & 0.1465 & 32.5049 & 0.3778 \\
0.21 & 1.0738 e+03 & 14.1178 & 0.1457 & 32.3324 & 0.3765 \\
0.2  & 1.0727 e+03 & 14.0599 & 0.1457 & 32.3137 & 0.3764 \\
0.19 & 1.0715 e+03 & 14.0000 & 0.1455 & 32.2937 & 0.3762 \\
0.18 & 1.0701 e+03 & 13.9371 & 0.1454 & 32.2713 & 0.3760 \\
0.17 & 1.0685 e+03 & 13.8707 & 0.1453 & 32.2461 & 0.3758 \\
0.16 & 1.0668 e+03 & 13.8006 & 0.1451 & 32.2180 & 0.3756 \\
0.15 & 1.0648 e+03 & 13.7253 & 0.1449 & 32.1870 & 0.3753 \\
0.14 & 1.0626 e+03 & 13.6450 & 0.1446 & 32.1531 & 0.3750 \\
0.1  & 1.0512 e+03 & 13.2350 & 0.1433 & 31.9753 & 0.37333\\
\hline\hline
\end{tabular}
\end{center}
\caption{Forecast error for different hyperparameter $\lambda$}
\label{testres}
\end{table}




To sum up, We prefer the value of $\lambda$ that has the lowest error measures. From table \ref{testres}, all the errors are decreasing as the shrinkage estimator becomes stronger (i.e. the value of lambda becomes smaller). In this case, the shrinkage hyperparameter $\lambda=0.1$ outperforms other values in our training steps. Therefore, the optimal value of the shrinkage parameter $lambda$ for the Minnesota prior in our Bayesian VAR model is $\lambda=0.1$. 





\newpage

## Sectoral Employment Multiplier Analysis 

I will also conduct a two-digit subsectoral employment multiplier analysis based on the estimated BVAR model and the 87 two-digit subsectoral data. 

At each time period, we have : 

$$
GR_T=\sum_{j=1}^{87} w_j\times {GR}_j
$$

where $w_j$ is the share of employment of two-digit subsector $j$ in the total employment, $GR_T$ is the growth rate in total employment and $GR_j$ is the growth rate in employment of two-digit subsector $j$. 

Due to the interconnection of macroeconomic two-digit subsectors, when a two-digit subsector $j$ has an increase in employment, in the long run, it may have spillover effect onto other two-digit subsectors, especially those that have high connections with the two-digit subsector $j$ (@anderson2020).

If this long-run effect is larger than the immediate effect, then this two-digit subsector will have positive spillover effect on total employment, and *vice versa*. I will use the interconnection of BVAR parameters to study two-digit subsectors which have strong positive spillovers for the total employment. Then, if the government makes policies to stimulate these two-digit subsectors, the total employment will recover more effectively.  

\newpage

