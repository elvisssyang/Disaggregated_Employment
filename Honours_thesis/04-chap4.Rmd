---
chapter: 4
knit: "bookdown::render_book"
---



# Methdology

## Proposed Model

I plan to use a Bayesian VARX model based on the method used in @anderson2020. The VARX model is particularly useful in modelling dynamic behaviours of the inter-variable relationships [@warsono2019]. In the model, each sector is affected by the lags of sectoral annual growth and a lag of the total employment growth. In addition to serving as an economy-wide factor, the inclusion of the lag of total employment growth ensures the model's self-consistency (e.g., forecasting coherence).


<!-- The lag of total employment growth is included to act as an economy-wide factor, and also ensures the self-consistency (e.g. forecasting coherence) to close the model.  -->



In many time series models, the number of lags is selected based on the patterns of the time series (i.e. seasonality, cycle, or trend). Four lags are typically used for quarterly data (see @anderson2020 and @stock2001). However, because of the high-dimensionality and relatively small sample size in my case, I will use one lag of 84 sectors and one lag of the total employment. 

Therefore, the suggested BVAR model is: 

$$
\begin{aligned}
\textbf{y}_t=\textbf{c}+\textbf{A}_1 \textbf{y}_{t-1}+\boldsymbol{\Gamma}{x}_{t-1}+\bf{u}_t
\end{aligned}
$$

where $\bf{y}_t$ is an $84\times1$ vector of two-digit subsectoral employment growth rate at time $t$  and $\bf{x}_{t-1}$ is a $1\times1$ vector stands for one lag on the growth rate of the total employment (this vector of variables are predetermined at time $t$), $\textbf{c}$ is a vector of constants, $\bf{A}_{1}$ is an $84\times84$ parameter matrix. $\boldsymbol{\Gamma}$ is an $84\times1$ matrix and $\bf{u}_t$ is a vector of reduced form errors with the mean equal to zero and independent variance $\bf{u}_t \sim (0,\boldsymbol{\Sigma})$. (See Appendix A for details)

In the proposed model, the use of seasonality unadjusted data allows the estimates to be coherent (i.e. the sum of subsectoral employment equals total employment). Moreover, the share of each subsector also changes endogenously as the employment changes over time. Therefore, even if we have one lag of the growth rate of total employment, there is no multicollinearity because the shares of subsectors change over time. 




## Prior and Shrinkage

By imposing prior beliefs on the parameters, a Bayesian VAR aids in overcoming the curse of high dimensionality[@banbura2010large]. I will estimate the employment dynamics using a the Bayesian VAR model by specifying a Minnesota type prior [e.g. @anderson2020; @litterman1986; @robertson1999vector], which is defined as follows: 



$$
\begin{aligned}\label{eq:1}
&E[a_{i}^{jk}] = E[\gamma_{i}^j]=0\\
\\
&Var[a_i^{jk}]= 
\begin{cases}
\frac{\lambda^2}{i^2},&j=k\\
\frac{\lambda^2}{i^2}\frac{\sigma^2_{j}}{\sigma^2_k},& otherwise
\end{cases}\\
\\
&Var[\gamma_i^{j}]=\frac{\lambda^2}{i^2}\frac{\sigma^2_{j}}{\sigma^2_e}
\end{aligned}
$$

where in the proposed model (see *Chapter 4.1*), the number of lag is $i=1$. Therefore, the $a_{1}^{jk}$ and $\gamma_{1}^{jk}$ are ${j,k}^{th}$ of $\bm{A_1}$ and $\bm{\Gamma_1}$ matrices. The degree of shrinkage is governed by $\lambda$, where $\frac{1}{i^2}$ down-weights the distant lags in general notation and the $\frac{\sigma_j^2}{\sigma_k^2}$ adjusts for different scale of the data. $\sigma^2_e$ is the variance after fitting an AR model on total employment growth. 




@banbura2010large also suggests a natural conjugate Normal-Inverse-Wishart prior, which retains the principle of Minnesota prior. This will greatly simplify the steps of adding Minnesota prior to the Bayesian VAR model. Its posterior moments can be calculated either analytically or by adding the dummy observations. I will use dummy observations to estimate the BVAR [@banbura2010large]. More details are provided in the Appendix.





## Selecting the hyperparameter of Minnesota Prior 

Specifically, the Minnesota prior has the following beliefs about the variances in our estimated one lag Bayesian VARX model: 

$$
\begin{aligned}
&Var[a_1^{jk}]= 
\begin{cases}
\lambda^2,&j=k\\
\frac{\lambda^2\sigma^2_{j}}{\sigma^2_k},& otherwise
\end{cases}\cdots(4.3.1)\\
\\
&Var[\gamma_1^{j}]=\frac{\lambda^2\sigma^2_{j}}{\sigma^2_e}\cdots(4.3.2)
\end{aligned}
$$

where $\lambda$ is a hyperparameter specified based on how far we will shrink the estimates and $\frac{\sigma^2_{j}}{\sigma^2_k}$ adjusts for the different scale of the data. To effectively scale the estimator $\gamma^j_1$ and $a_j^{jk}$, I obtain $\sigma_n^2$ by fitting an AR(4) model on the $n$-th variable using least squares, which is commonly used in many literatures [@anderson2020;@banbura2010large;@koop2013]. 



As the Minnesota prior is defined from the previous equations (see $4.3.1$ and $4.3.2$), the hyperparameter $\lambda$ controls the overall tightness (variance) of the prior distribution [@banbura2010large]. If $\lambda\rightarrow0$, we can see that the prior assumption is influential, which means that the posterior is approaching to the prior. That is, the data do not affect the estimation. On the other hand, if $\lambda\rightarrow\infty$, the posterior expectations will approach the ordinary least squares (OLS) estimates. In many macroeconomic VAR forecasting scenarios, the data has a large dimension. As the dimension grows, we want to shrink more in order to prevent over-fitting [@de2008]. 






Undoubtedly, by regulating the degree of shrinkage, the hyperparameter $\lambda$ is crucial in increasing forecast accuracy. For example, @banbura2010large points out that a gain in efficiency could be made by applying  Bayesian shrinkage in estimating large multivariate VAR models. Additionally, they also conclude that large Bayesian vector autoregressions (BVARs) with shrinkage are helpful for constructing structural analyses.



Based on applied experience, Litterman came to the conclusion that the shrinkage estimate $lambda=0.2$ is adequate to handle a large number of empirical cases [@litterman1986]. More importantly, the data size needs to be considered as well when determining the degree of shrinkage [@banbura2010large]. The two-digit level (84 subsectors) of employment in Australia is more complex with 84 subsectors than the one-digit level (19 sectors) studied by @anderson2020. Therefore, $\lambda=0.2$ might not be appropriate in this multivariate case. Here, I will give a thorough breakdown of the approach I employed to select the optimal $\lambda$.








Due to the fact that disaggregated subsectors have different scales, the commonly used scale-dependent error measurement (e.g. MAE, MSE) may not work when comparing forecast accuracy between subsectors. Despite being unit-free, mean absolute percentage error (MAPE) is inaccurate in subsectors with relatively small shares (e.g. a small change in a relative small subsector will significantly increase the MAPE for that subsector). When $y_t$ is close to zero, MAPE will likely have extreme values or become undefined. Accordingly, I will sum the employment across all sectors to total employment and minimise the total employment forecast error to select the optimal $\lambda$.





The error measurement I will use is the root mean squared forecast error RMSFE. It is calculated via an out-of-sample forecasting experiment, which is a similar approach used in many empirical cases [@banbura2010large;@koop2013]. Here, I denote $H$ as the longest forecast horizon to be evaluated, both $T_{b}$ and $T_{e}$ as the length of the training set and testing set, respectively. Give the forecast horizon $h$, hyperparameter $\lambda$ and model $m$, for each given period between $T_{b}$ and $T_{e}$ ($T=T_b,\cdots,T_{e}-h$), I compute $h$-step-ahead forecasts ${y}_{i,T+h|T}^{(\lambda,m)}$ using only the information up to time T. I then compute the forecast error $y_{i,T+h}$ by subtracting the actual data $y_{i,T+h}$.



Then, out-of-sample forecast accuracy is measured in terms of the root mean squared forecast error (**RMSFE**) as: 


$$
\begin{aligned}
RMSFE^{\lambda}_{h}=\sqrt{\frac{1}{T_e-h-T_b+1}\Sigma^{T_{e}-h}_{T=T_{b}}({y}_{T+h|T}^{\lambda}-y_{T+h})^2}
\end{aligned}
$$


where ${y}_{T+h|T}^{\lambda}$ is defined as the $h$-th steps ahead forecast (total employment in this case) given the information up to time $T$ and $y_{T+h}$ is the actual data for the $h$-th steps ahead forecast (total employment in this case). Here, $\lambda$ stands for the evaluated RMSFE, conditioned on a specific model and the hyperparameter $\lambda$. 



In this section, I will set up an effective searching algorithm to search for the optimal shrinkage parameter $\lambda$. For our purposes, I want to provide accurate forecasts of total employment based on the scenario in which there was no pandemic happened to conduct the counterfactual analysis. Therefore, the pre-covid total employment data (before 2020 Quarter 2) of length $T_e=142$ is divided into training and test portions, with a training set length ($n=120=T_b$) and a test set length ($H=22=T_e-h-T_b+1$). In addition, I use a one-step forecasting experiment with the $h=1$. 




Here is a brief description of the proposed algorithm: 



\graphicspath{ {/Users/elvisyang/Desktop/hon_proj/Disaggregated_Employment/Honours_thesis/figures} }

\begin{figure}[ht]
\includegraphics[scale=0.7]{Flowchart_algo}
\centering
\caption{Proposed algorithm for selecting the optimal $\lambda$}
\label{fig:sealgo}
\end{figure}
  



To mitigate the adverse impact of high-dimensionality, I start the algorithm from $0.0001$, use steps of $0.0001$ and stop at $0.3$. There are 3000 different lambdas considered, and the algorithm will automatically return the lambda with minimum RMSFE in forecasting total number of employment (see Matlab code for this algorithm in my github file-Links in the Appendix ). 



From the return value of our searching algorithm (see Figure \ref{fig:sealgo}). The estimated hyperparameter from the algorithm is $\lambda=0.0808$, which certainly has the lowest mean scaled forecast error (RMSFE) as designed. Based on this, I choose $\lambda=0.0808$ for subsequent analysis. 



\newpage

