---
chapter: 4
knit: "bookdown::render_book"
---



# Methdology

## Proposed Model

I plan to use a Bayesian VARX model based on the method used in @anderson2020. The VARX model is especially useful in modelling dynamic behaviours of the relationships between variables [@warsono2019]. In the model, each sector is affected by the lags of sectoral annual growth and a lag of the total employment growth. The lag of total employment growth is included to act as an economy-wide factor, and also ensures the self-consistency (e.g. forecasting coherence) to close the model. 



In many time series models, the number of lags is selected according to the patterns of the time series (i.e. seasonality, cycle, or trend). For quarterly data, four lags are usually used (see @anderson2020 and @stock2001). However, because of the high-dimensionality and relatively small sample size in my case, I will use one lag of 84 sectors and one lag of the total employment. 

Therefore, the suggested BVAR model is: 

$$
\begin{aligned}
\textbf{y}_t=\textbf{c}+\textbf{A}_1 \textbf{y}_{t-1}+\boldsymbol{\Gamma}{x}_{t-1}+\bf{u}_t
\end{aligned}
$$

where $\bf{y}_t$ is an $84\times1$ vector of two-digit subsectoral employment growth rate at time $t$  and $\bf{x}_{t-1}$ is a $1\times1$ vector stands for one lag on the growth rate of the total employment (this vector of variables are predetermined at time $t$), $\textbf{c}$ is a vector of constants, $\bf{A}_{1}$ is an $84\times84$ parameter matrix. $\boldsymbol{\Gamma}$ is an $84\times1$ matrix and $\bf{u}_t$ is a vector of reduced form errors with the mean equal to zero and independent variance $\bf{u}_t \sim (0,\boldsymbol{\Sigma})$. (See Appendix A for details)

In the proposed model, the use of seasonality unadjusted data allows the estimates to be coherent (i.e. the sum of subsectoral employment equals total employment). Moreover, the share of each subsector changes endogenously as the varying employment over time. Therefore, even if we have one lag of the growth rate of total employment, there is no multicollinearity because the shares of subsectors change over time. 




## Prior and Shrinkage

A Bayesian VAR helps to overcome the curse of high dimensionality by imposing prior beliefs on the parameters [@banbura2010large]. I will estimate the employment dynamics using a the Bayesian VAR model by specifying a Minnesota type prior [e.g. @anderson2020; @litterman1986; @robertson1999vector], which is defined as follows: 



$$
\begin{aligned}\label{eq:1}
&E[a_{i}^{jk}] = E[\gamma_{i}^j]=0\\
\\
&Var[a_i^{jk}]= 
\begin{cases}
\frac{\lambda^2}{i^2},&j=k\\
\frac{\lambda^2}{i^2}\frac{\sigma^2_{j}}{\sigma^2_k},& otherwise
\end{cases}\\
\\
&Var[\gamma_i^{j}]=\frac{\lambda^2}{i^2}\frac{\sigma^2_{j}}{\sigma^2_e}
\end{aligned}
$$

where in the proposed model (see **Chapter 4.1**), the number of lag is $i=1$. Therefore, the $a_{1}^{jk}$ and $\gamma_{1}^{jk}$ are ${j,k}^{th}$ of $A_1$ and $\Gamma_1$ matrices,  degree of shrinkage is governed by $\lambda$, $\frac{1}{i^2}$ down-weights the distant lags and the $\frac{\sigma_j^2}{\sigma_k^2}$ adjusts for different scale of the data. $\sigma^2_e$ is the variance after fitting an AR model on total employment growth. 




@banbura2010large also suggests a natural conjugate Normal-Inverse-Wishart prior, which retains the principle of Minnesota prior. This will greatly simplify the steps of adding Minnesota prior to the Bayesian VAR model. Its posterior moments can be calculated either analytically or by adding the dummy observations. I will use dummy observations to estimate the BVAR [@banbura2010large]. More details are provided in the Appendix.





## Selecting the hyperparameter of Minnesota Prior 

Specifically, the Minnesota type prior has the following beliefs about the variances in our estimated one lag BVAR model: 

$$
\begin{aligned}
&Var[a_1^{jk}]= 
\begin{cases}
\lambda^2,&j=k\\
\frac{\lambda^2\sigma^2_{j}}{\sigma^2_k},& otherwise
\end{cases}\cdots(4.3.1)\\
\\
&Var[\gamma_1^{j}]=\frac{\lambda^2\sigma^2_{j}}{\sigma^2_e}\cdots(4.3.2)
\end{aligned}
$$

where $\lambda$ is a hyperparameter specified based on how far we will shrink the estimates and $\frac{\sigma^2_{j}}{\sigma^2_k}$ adjusts for the different scale of the data. To effectively scale the estimator $\gamma^j_1$ and $a_j^{jk}$, I obtain $\sigma_n^2$ by fitting an AR(4) model on the $n$-th variable using least squares, which is commonly used in many literatures [@anderson2020;@banbura2010large;@koop2013]. 



As the Minnesota prior defined from the above equations (see $4.3.1$ and $4.3.2$), the hyperparameter $\lambda$ controls the overall tightness (variance) of the prior distribution [@banbura2010large]. If $\lambda\rightarrow0$, we can see that the prior assumption is influential, which means that the posterior getting closer to the prior. That is, the data do not affect the estimation. In contrast, if $\lambda\rightarrow\infty$, the posterior expectations will approach the ordinary least squares (OLS) estimates. In many macroeconomic VAR forecasting situations, the data has a large dimension. As the dimension increases, we want to shrink more in order to avoid over-fitting [@de2008]. 






Admittedly, the hyperparameter $\lambda$ plays an important role in improving forecast accuracy by controlling the degree of shrinkage. For example, @banbura2010large point out that a gain in efficiency could be made by applying  Bayesian shrinkage in estimating large multivariate VAR models. They also conclude that large bayesian vector autoregressions (BVARs) with shrinkage are useful for constructing structural analysis.






Based on applied experience, Litterman concluded that the shrinkage estimate $\lambda=0.2$ is sufficient to deal with many empirical cases [@litterman1986]. More importantly, the data size should also needs to be considered as well when deciding the degree of shrinkage [@banbura2010large]. Unlike the one-digit level (19 sectors) studied by @anderson2020, the two-digit level (84 subsectors) employment in Australia is more complex on many variables. So $\lambda=0.2$ may not be suitable for this multivariate case. Here, I will provide a detailed analysis of the approach I used to select the optimal $\lambda$.








Due to the reason that the size of disaggregated subsectors has different scales, the commonly used scale-dependent error measurement (e.g. MAE, MSE) may fail when comparing forecast accuracy between subsectors. Even though MAPE is unit-free, it is not accurate in sectors that have relatively small shares (e.g. a small change will significantly increase the MAPE). When $y_t$ is close to zero, MAPE will likely have extreme values or become undefined. Accordingly, I will sum all sectors to the total employment and minimise the forecast error of total employment to select the optimal $\lambda$.



The error measurement I will use is the root mean squared forecast error RMSFE. It is calculated via an out-of-sample forecasting experiment, which is similar to practice in many empirical cases [@banbura2010large;@koop2013]. Here, I denote $H$ as the longest forecast horizon to be evaluated, both $T_{b}$ and $T_{e}$ as the end of the training set and testing set, respectively. Give the forecast horizon $h$, hyperparameter $\lambda$ and model $m$, for each given period between $T_{b}$ and $T_{e}$ ($T=T_b,\cdots,T_{e}-h$), I compute $h$-step-ahead forecasts ${y}_{i,T+h|T}^{(\lambda,m)}$, using only the information up to time T. I then minus the actual data $y_{i,T+h}$ to calculate the forecast error.



Then, out-of-sample forecast accuracy is measured in terms of the root mean squared forecast error (**RMSFE**) as: 


$$
\begin{aligned}
RMSFE^{\lambda}_{h}=\sqrt{\frac{1}{T_e-h-T_b+1}\Sigma^{T_{e}-h}_{T=T_{b}}({y}_{T+h|T}^{\lambda}-y_{T+h})^2}
\end{aligned}
$$


where ${y}_{T+h|T}^{\lambda}$ is defined as the $h$-th steps ahead forecast (total employment in this case) given the information up to time $T$ and $y_{T+h}$ is the actual data for the $h$-th steps ahead forecast (total employment in this case). Here, $\lambda$ stands for the evaluated RMSFE, conditioned on a specific model and the hyperparameter $\lambda$. 



In this section, I will set up an effective searching algorithm to search for the optimal shrinkage estimator $\lambda$. For our purposes, I want to provide accurate forecasts of total employment based on the scenario where no covid happened to support our counterfactual analysis. Therefore, the pre-covid total employment data (before 2020 Quarter 2) of length $T_e=142$ is split into training and test portions with a training set of length ($n=120=T_b$) and a test set of length ($n=22=T_e-h-T_b+1$). As a consequence, I will set $H=22$ to be the length of test set, $h=1$ for a one step experiment. 


Here is a brief description of the proposed algorithm: 



\graphicspath{ {/Users/elvisyang/Desktop/hon_proj/Disaggregated_Employment/Honours_thesis/figures} }

\begin{figure}[ht]
\includegraphics[scale=0.7]{Flowchart_algo}
\centering
\caption{Proposed algorithm for selecting the optimal $\lambda$}
\label{fig:sealgo}
\end{figure}
  



To mitigate the adverse impact of high-dimensionality, I set the algorithm to start from $0.0001$, use steps of $0.0001$ and stop at $0.3$. There are 3000 different lambdas considered, and the algorithm will automatically return the lambda with minimum RMSFE in forecasting total number of employment (see Matlab code in my github [Link https://github.com/elvisssyang/Disaggregated_Employment/tree/main/Matlab]). 



From the return value of our searching algorithm (see Figure \ref{fig:sealgo}). The estimated hyperparameter from the algorithm is $\lambda=0.0808$, which certainly has the lowest mean scaled forecast error (RMSFE) as designed. Based on this, I choose $\lambda=0.0808$ for subsequent analysis. 



\newpage

